{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: Xgboost package is not installed. You will be unable to use the xgboost decoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 19:15:21.847011: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-08 19:15:22.816205: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-08 19:15:22.816337: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-08 19:15:22.880602: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-08 19:15:24.508724: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-08 19:15:24.509206: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-08 19:15:24.509220: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../code/') \n",
    "sys.path.append('../externals/transformer/') \n",
    "from tst import Transformer\n",
    "\n",
    "import mocap_functions\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import neo\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "from scipy import signal\n",
    "from scipy.interpolate import interp1d\n",
    "import spike_train_functions\n",
    "import elephant\n",
    "import quantities as pq\n",
    "# import h5py\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import Neural_Decoding\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from hnn_core.utils import smooth_waveform\n",
    "from scipy.signal import savgol_filter\n",
    "#sns.set()\n",
    "#sns.set_style(\"white\")\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "#device = torch.device(\"cuda:0\")\n",
    "device = torch.device('cpu')\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_idx = 1\n",
    "kinematic_df, neural_df, metadata = mocap_functions.load_mocap_df('../data/SPK20220308/task_data/', kinematic_suffix=f'_cam{cam_idx}')\n",
    "num_trials = len(kinematic_df['trial'].unique())\n",
    "\n",
    "null_percent = kinematic_df.groupby('name')['posData'].apply(list).map(\n",
    "    np.concatenate).map(lambda x: np.sum(np.isnan(x)) / len(x))\n",
    "\n",
    "pos_filter = [1, 4]\n",
    "pos_remove_filter = [f'position_{pos_idx}' for pos_idx in [2,3]]\n",
    "# layout_filter = [1,2,3,4]\n",
    "layout_filter = [1,2]\n",
    "\n",
    "layout_remove_filter = [f'layout_{layout_idx}' for layout_idx in []]\n",
    "\n",
    "\n",
    "neural_df = neural_df[np.in1d(neural_df['position'], pos_filter)].reset_index(drop=True)\n",
    "neural_df = neural_df[np.in1d(neural_df['layout'], layout_filter)].reset_index(drop=True)\n",
    "neural_df = neural_df[~np.in1d(neural_df['unit'], pos_remove_filter)].reset_index(drop=True)\n",
    "neural_df = neural_df[~np.in1d(neural_df['unit'], layout_remove_filter)].reset_index(drop=True)\n",
    "\n",
    "kinematic_df = kinematic_df[np.in1d(kinematic_df['position'], pos_filter)].reset_index(drop=True)\n",
    "kinematic_df = kinematic_df[np.in1d(kinematic_df['layout'], layout_filter)].reset_index(drop=True)\n",
    "kinematic_df = kinematic_df[~np.in1d(kinematic_df['name'], pos_remove_filter)].reset_index(drop=True)\n",
    "kinematic_df = kinematic_df[~np.in1d(kinematic_df['name'], layout_remove_filter)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subselect specific marker\n",
    "# marker_list = ['ulnarDistal', 'carpal', 'thumbProx', 'ringProx','pinkyProx'] # cam4\n",
    "marker_list = ['ringProx', 'pinkyProx', 'middleProx'] # cam4\n",
    "# marker_list = ['indexProx', 'carpal', 'ringProx'] # cam1\n",
    "\n",
    "\n",
    "mask_list = [kinematic_df['name'].str.contains(pat=pat) for pat in marker_list]\n",
    "wrist_df = kinematic_df[np.logical_or.reduce(mask_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove trials where marker velocity exceeds 3x std\n",
    "outlier_thresh = 6\n",
    "velocity_std = np.concatenate(wrist_df['posData'].map(np.diff).values).std()\n",
    "outlier_mask = wrist_df['posData'].map(np.diff).apply(\n",
    "    lambda x: np.any(np.abs(x - np.mean(x)) > outlier_thresh * velocity_std))\n",
    "outlier_trials = wrist_df[outlier_mask]['trial'].unique()\n",
    "\n",
    "wrist_df = wrist_df[wrist_df['trial'].apply(lambda x: x not in outlier_trials)]\n",
    "neural_df = neural_df[neural_df['trial'].apply(lambda x: x not in outlier_trials)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array_equal(neural_df['trial'].unique(), wrist_df['trial'].unique())\n",
    "trial_ids = neural_df['trial'].unique()\n",
    "\n",
    "num_trials_filtered = len(trial_ids)\n",
    "\n",
    "#Generate cv_dict for regular train/test/validate split (no rolling window)\n",
    "cv_split = ShuffleSplit(n_splits=5, test_size=.25, random_state=3)\n",
    "val_split = ShuffleSplit(n_splits=1, test_size=.25, random_state=3)\n",
    "cv_dict = {}\n",
    "for fold, (train_val_idx, test_idx) in enumerate(cv_split.split(trial_ids)):\n",
    "    for t_idx, v_idx in val_split.split(train_val_idx): #No looping, just used to split train/validation sets\n",
    "        cv_dict[fold] = {'train_idx':trial_ids[train_val_idx[t_idx]], \n",
    "                         'test_idx':trial_ids[test_idx], \n",
    "                         'validation_idx':trial_ids[train_val_idx[v_idx]]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neurons = 5\n",
    "rng = np.random.default_rng(222)\n",
    "random_units = rng.choice(range(85), size=85).astype(str)\n",
    "\n",
    "unit_mask = np.in1d(neural_df['unit'].values, random_units[:num_neurons])\n",
    "layout_mask = neural_df['unit'].str.contains(pat='layout')\n",
    "position_mask = neural_df['unit'].str.contains(pat='position')\n",
    "handpos_mask = neural_df['unit'].str.contains(pat='handpos')\n",
    "\n",
    "# neural_df = neural_df[np.logical_or.reduce([unit_mask, layout_mask, position_mask])].reset_index(drop=True)\n",
    "neural_df = neural_df[np.logical_or.reduce([unit_mask, layout_mask])].reset_index(drop=True)\n",
    "# neural_df = neural_df[np.logical_or.reduce([unit_mask, position_mask])].reset_index(drop=True)\n",
    "# neural_df = neural_df[np.logical_or.reduce([unit_mask, handpos_mask])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth everything after adding noise\n",
    "smooth_func = partial(savgol_filter, window_length=31, polyorder=3)\n",
    "neural_df['rates'] = neural_df['rates'].map(smooth_func)\n",
    "wrist_df['posData'] = wrist_df['posData'].map(smooth_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dataframes add or remove layout info\n",
    "nolayout_kinematic_mask = ~(kinematic_df['name'].str.contains(pat='layout'))\n",
    "noposition_kinematic_mask = ~(kinematic_df['name'].str.contains(pat='position'))\n",
    "\n",
    "eye_kinematic_mask = kinematic_df['name'].str.contains(pat='eye')\n",
    "corneal_kinematic_mask = kinematic_df['name'].str.contains(pat='corneal')\n",
    "\n",
    "temp_eye_df = kinematic_df[np.logical_or(eye_kinematic_mask, corneal_kinematic_mask)]\n",
    "kinematic_df = kinematic_df[np.logical_and.reduce([nolayout_kinematic_mask, noposition_kinematic_mask, ~eye_kinematic_mask, ~corneal_kinematic_mask])]\n",
    "\n",
    "nolayout_neural_mask = ~(neural_df['unit'].str.contains(pat='layout'))\n",
    "noposition_neural_mask = ~(neural_df['unit'].str.contains(pat='position'))\n",
    "nohandpos_neural_mask = ~(neural_df['unit'].str.contains(pat='handpos'))\n",
    "notask_neural_df = neural_df[np.logical_and.reduce([nolayout_neural_mask, noposition_neural_mask, nohandpos_neural_mask])]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eye_data = [[data] for data in temp_eye_df['posData'].tolist()]\n",
    "eye_dict = {'rates': temp_eye_df['posData'].tolist(), 'unit': temp_eye_df['name'].tolist(), 'trial': temp_eye_df['trial'].tolist(),\n",
    "            'layout': temp_eye_df['layout'].tolist(), 'count': np.repeat(0.0, len(temp_eye_df))}\n",
    "\n",
    "eye_df = pd.DataFrame(eye_dict).reset_index()\n",
    "\n",
    "task_neural_noeyes_df = neural_df.copy()\n",
    "notask_neural_noeyes_df = notask_neural_df.copy()\n",
    "task_neural_eyes_df = pd.concat([eye_df, neural_df])\n",
    "notask_neural_eyes_df = pd.concat([eye_df, notask_neural_df])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRU architecture for decoding kinematics\n",
    "class model_gru(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, dropout, device, bidirectional=False,\n",
    "                 cat_features=None):\n",
    "        super(model_gru, self).__init__()\n",
    "\n",
    "        #multiplier based on bidirectional parameter\n",
    "        if bidirectional:\n",
    "            num_directions = 2\n",
    "        else:\n",
    "            num_directions = 1\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim       \n",
    "        self.n_layers = n_layers * num_directions\n",
    "        self.device = device\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        self.cat_features = cat_features\n",
    "        self.input_size = input_size\n",
    "\n",
    "        if self.cat_features is not None:\n",
    "            self.num_cat_features = np.sum(self.cat_features).astype(int)\n",
    "            self.hidden_fc = nn.Linear(self.num_cat_features, self.hidden_dim)\n",
    "\n",
    "            self.input_size = self.input_size - self.num_cat_features\n",
    "\n",
    "            \n",
    "        else:\n",
    "            self.fc = nn.Linear(self.hidden_dim * num_directions, output_size)\n",
    "\n",
    "        self.fc = nn.Linear((self.hidden_dim* num_directions), output_size)\n",
    "        self.gru = nn.GRU(self.input_size, self.hidden_dim, n_layers, batch_first=True, dropout=dropout, bidirectional=bidirectional) \n",
    "\n",
    "      \n",
    "\n",
    "        #Defining the layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        if self.cat_features is not None:\n",
    "            cat_hidden = self.hidden_fc(torch.tanh(x[:, -1, self.cat_features]))\n",
    "            hidden = hidden + cat_hidden\n",
    "            out, hidden = self.gru(x[:, :, ~self.cat_features], hidden)\n",
    "            out = out.contiguous()\n",
    "\n",
    "        else:\n",
    "            out, hidden = self.gru(x, hidden)\n",
    "            out = out.contiguous()\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data.to(self.device)\n",
    "\n",
    "        #GRU initialization\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(self.device)\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generators(pred_df, neural_df, neural_offset, cv_dict, metadata,\n",
    "                    exclude_neural=None, exclude_kinematics=None, window_size=1, \n",
    "                    flip_outputs=False, fold=0, batch_size=10000, device='cpu',):\n",
    "    sampling_rate = 100\n",
    "    kernel_offset = int(metadata['kernel_halfwidth'] * sampling_rate)  #Convolution kernel centered at zero, add to neural offset\n",
    "    offset = neural_offset + kernel_offset\n",
    "    data_step_size = 1 \n",
    "\n",
    "    # Set up PyTorch Dataloaders\n",
    "    \n",
    "    # Parameters\n",
    "    train_params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': num_cores, 'pin_memory':False}\n",
    "    train_eval_params = {'batch_size': batch_size, 'shuffle': False, 'num_workers': num_cores, 'pin_memory':False}\n",
    "    validation_params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': num_cores, 'pin_memory':False}\n",
    "    test_params = {'batch_size': batch_size, 'shuffle': False, 'num_workers': num_cores, 'pin_memory':False}\n",
    "\n",
    "    scale_neural = True\n",
    "    scale_kinematics = True\n",
    "    flip_outputs=flip_outputs\n",
    "\n",
    "    # Generators\n",
    "    training_set = SEE_Dataset_packed(cv_dict, fold, 'train_idx', pred_df, neural_df, offset, window_size, \n",
    "                               data_step_size, device, 'posData', scale_neural=scale_neural,\n",
    "                               scale_kinematics=scale_kinematics, flip_outputs=flip_outputs,\n",
    "                               exclude_neural=exclude_neural, exclude_kinematic=exclude_kinematics)\n",
    "    training_neural_scaler = training_set.neural_scaler\n",
    "    training_kinematic_scaler = training_set.kinematic_scaler\n",
    "\n",
    "    training_generator = torch.utils.data.DataLoader(training_set, **train_params)\n",
    "    training_eval_generator = torch.utils.data.DataLoader(training_set, **train_eval_params)\n",
    "\n",
    "    validation_set = SEE_Dataset_packed(cv_dict, fold, 'validation_idx', pred_df, neural_df, offset, window_size, \n",
    "                                 data_step_size, device, 'posData', scale_neural=scale_neural,\n",
    "                                 scale_kinematics=scale_kinematics, flip_outputs=flip_outputs,\n",
    "                                 exclude_neural=exclude_neural, exclude_kinematic=exclude_kinematics,\n",
    "                                 neural_scaler=training_neural_scaler, kinematic_scaler=training_kinematic_scaler)\n",
    "    validation_generator = torch.utils.data.DataLoader(validation_set, **validation_params)\n",
    "\n",
    "    testing_set = SEE_Dataset_packed(cv_dict, fold, 'test_idx', pred_df, neural_df, offset, window_size, \n",
    "                              data_step_size, device, 'posData', scale_neural=scale_neural,\n",
    "                              scale_kinematics=scale_kinematics, flip_outputs=flip_outputs,\n",
    "                              exclude_neural=exclude_neural, exclude_kinematic=exclude_kinematics,\n",
    "                              neural_scaler=training_neural_scaler, kinematic_scaler=training_kinematic_scaler)\n",
    "    testing_generator = torch.utils.data.DataLoader(testing_set, **test_params)\n",
    "\n",
    "    data_arrays = (training_set, validation_set, testing_set)\n",
    "    generators = (training_generator, training_eval_generator, validation_generator, testing_generator)\n",
    "\n",
    "    return data_arrays, generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset class to handle mocap dataframes from SEE project\n",
    "class SEE_Dataset_packed(torch.utils.data.Dataset):\n",
    "    #'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, cv_dict, fold, partition, kinematic_df, neural_df, offset, device,\n",
    "                 kinematic_type='posData', scale_neural=True, scale_kinematics=True, flip_outputs=False,\n",
    "                 exclude_neural=None, exclude_kinematic=None, neural_scaler=None, kinematic_scaler=None):\n",
    "        #'Initialization'\n",
    "        self.cv_dict = cv_dict\n",
    "        self.fold = fold\n",
    "        self.flip_outputs = flip_outputs\n",
    "        self.partition = partition\n",
    "        self.trial_idx = cv_dict[fold][partition]\n",
    "        self.num_trials = len(self.trial_idx) \n",
    "        self.offset = offset\n",
    "        self.device = device\n",
    "        self.posData_list, self.neuralData_list = self.process_dfs(kinematic_df, neural_df)\n",
    "        self.seq_lengths = [self.posData_list[idx].shape[0] for idx in range(self.num_trials)]\n",
    "        if neural_scaler is None:\n",
    "            neural_scaler = StandardScaler()\n",
    "            if exclude_neural is not None:\n",
    "                neural_scaler.fit(np.vstack(self.neuralData_list)[:, ~exclude_neural])\n",
    "            else:\n",
    "                neural_scaler.fit(np.vstack(self.neuralData_list))\n",
    "        self.neural_scaler = neural_scaler\n",
    "        \n",
    "        if kinematic_scaler is None:\n",
    "            kinematic_scaler = StandardScaler()\n",
    "            if exclude_kinematic is not None:\n",
    "                kinematic_scaler.fit(np.vstack(self.posData_list)[:, ~exclude_kinematic])\n",
    "            else:\n",
    "                kinematic_scaler.fit(np.vstack(self.posData_list))\n",
    "        self.kinematic_scaler = kinematic_scaler\n",
    "\n",
    "        # Boolean array of 1's for features to not be scaled\n",
    "        if scale_kinematics:\n",
    "            self.posData_list = self.transform_data(self.posData_list, self.kinematic_scaler, exclude_kinematic)\n",
    "        \n",
    "        if scale_neural:\n",
    "            self.neuralData_list = self.transform_data(self.neuralData_list, self.neural_scaler, exclude_neural)\n",
    "\n",
    "        self.kinematic_type = kinematic_type\n",
    "\n",
    "        self.num_samples = None\n",
    "        self.X_tensor, self.y_tensor = self.load_splits()\n",
    "\n",
    "    def __len__(self):\n",
    "        #'Denotes the total number of samples'\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, slice_index):\n",
    "        X, y = self.X_tensor[slice_index], self.y_tensor[slice_index]\n",
    "  \n",
    "        return X, y\n",
    "    #**add functionality to separate eye, object, and body markers\n",
    "    def process_dfs(self, kinematic_df, neural_df):\n",
    "        posData_list, neuralData_list = [], []\n",
    "        for trial in self.trial_idx:\n",
    "            posData_array = np.stack(kinematic_df['posData'][kinematic_df['trial'] == trial].values).transpose() \n",
    "            neuralData_array = np.stack(neural_df['rates'][neural_df['trial'] == trial].values).squeeze().transpose() \n",
    "\n",
    "            posData_list.append(posData_array)\n",
    "            neuralData_list.append(neuralData_array)\n",
    "\n",
    "        return posData_list, neuralData_list\n",
    "\n",
    "    def format_splits(self, data_list):\n",
    "        data_tensor = [torch.tensor(data_list[idx]) for idx in range(self.num_trials)]\n",
    "      \n",
    "        return data_tensor\n",
    "    \n",
    "    def load_splits(self):\n",
    "        if not self.flip_outputs:\n",
    "            X_tensor = self.format_splits(self.posData_list)\n",
    "            y_tensor = self.format_splits(self.neuralData_list)\n",
    "        else:\n",
    "            y_tensor = self.format_splits(self.posData_list)\n",
    "            X_tensor = self.format_splits(self.neuralData_list)\n",
    "\n",
    "     \n",
    "        assert len(X_tensor) == len(y_tensor)\n",
    "        self.num_samples = len(X_tensor)\n",
    "        return X_tensor, y_tensor\n",
    "\n",
    "    #Zero mean and unit std\n",
    "    def transform_data(self, data_list, scaler, exclude_processing):\n",
    "        #Iterate over trials and apply normalization\n",
    "     \n",
    "        scaled_data_list = []\n",
    "        for data_trial in data_list:\n",
    "            if exclude_processing is None:\n",
    "                scaled_data_trial = scaler.transform(data_trial)\n",
    "            else:\n",
    "                scaled_data_trial = np.zeros(data_trial.shape)\n",
    "                scaled_data_trial[:, exclude_processing] = data_trial[:, exclude_processing]\n",
    "                processed_data = scaler.transform(data_trial[:, ~exclude_processing])\n",
    "                scaled_data_trial[:, ~exclude_processing] = processed_data\n",
    "            scaled_data_list.append(scaled_data_trial)\n",
    "\n",
    "        return scaled_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://suzyahyah.github.io/pytorch/2019/07/01/DataLoader-Pad-Pack-Sequence.html\n",
    "def pad_collate(batch):\n",
    "    (xx, yy) = zip(*batch)\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    y_lens = [len(y) for y in yy]\n",
    "\n",
    "    xx_pad = torch.nn.utils.rnn.pad_sequence(xx, batch_first=True, padding_value=0)\n",
    "    yy_pad = torch.nn.utils.rnn.pad_sequence(yy, batch_first=True, padding_value=0)\n",
    "    return xx_pad, yy_pad\n",
    "    # return xx_pad, yy_pad, x_lens, y_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_offset = 2\n",
    "window_size = 10\n",
    "exclude_kinematics = None\n",
    "exclude_neural = None\n",
    "# data_arrays, generators = mocap_functions.make_generators(\n",
    "# wrist_df, neural_df, neural_offset, cv_dict, metadata, exclude_neural=None,\n",
    "# flip_outputs=True)\n",
    "\n",
    "# # Unpack tuple into variables\n",
    "# training_set, validation_set, testing_set = data_arrays\n",
    "# training_generator, training_eval_generator, validation_generator, testing_generator = generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 100\n",
    "kernel_offset = int(metadata['kernel_halfwidth'] * sampling_rate)  #Convolution kernel centered at zero, add to neural offset\n",
    "offset = neural_offset + kernel_offset\n",
    "data_step_size = 1 \n",
    "\n",
    "# Set up PyTorch Dataloaders\n",
    "batch_size=100\n",
    "# Parameters\n",
    "train_params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': num_cores, 'pin_memory':False, 'collate_fn': pad_collate}\n",
    "train_eval_params = {'batch_size': batch_size, 'shuffle': False, 'num_workers': num_cores, 'pin_memory':False, 'collate_fn': pad_collate}\n",
    "validation_params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': num_cores, 'pin_memory':False, 'collate_fn': pad_collate}\n",
    "test_params = {'batch_size': batch_size, 'shuffle': False, 'num_workers': num_cores, 'pin_memory':False, 'collate_fn': pad_collate}\n",
    "\n",
    "scale_neural = True\n",
    "scale_kinematics = True\n",
    "flip_outputs=True\n",
    "full_sequence=True\n",
    "\n",
    "training_set = SEE_Dataset_packed(cv_dict, fold, 'train_idx', wrist_df, neural_df, offset,\n",
    "                            device, 'posData', scale_neural=scale_neural,\n",
    "                            scale_kinematics=scale_kinematics, flip_outputs=flip_outputs,\n",
    "                            exclude_neural=exclude_neural, exclude_kinematic=exclude_kinematics)\n",
    "training_neural_scaler = training_set.neural_scaler\n",
    "training_kinematic_scaler = training_set.kinematic_scaler\n",
    "\n",
    "training_generator = torch.utils.data.DataLoader(training_set, **train_params)\n",
    "training_eval_generator = torch.utils.data.DataLoader(training_set, **train_eval_params)\n",
    "\n",
    "validation_set = SEE_Dataset_packed(cv_dict, fold, 'validation_idx', wrist_df, neural_df, offset,\n",
    "                                device, 'posData', scale_neural=scale_neural,\n",
    "                                scale_kinematics=scale_kinematics, flip_outputs=flip_outputs,\n",
    "                                exclude_neural=exclude_neural, exclude_kinematic=exclude_kinematics,\n",
    "                                neural_scaler=training_neural_scaler, kinematic_scaler=training_kinematic_scaler)\n",
    "validation_generator = torch.utils.data.DataLoader(validation_set, **validation_params)\n",
    "\n",
    "testing_set = SEE_Dataset_packed(cv_dict, fold, 'test_idx', wrist_df, neural_df, offset,\n",
    "                                device, 'posData', scale_neural=scale_neural,\n",
    "                                scale_kinematics=scale_kinematics, flip_outputs=flip_outputs,\n",
    "                                exclude_neural=exclude_neural, exclude_kinematic=exclude_kinematics,\n",
    "                                neural_scaler=training_neural_scaler, kinematic_scaler=training_kinematic_scaler)\n",
    "testing_generator = torch.utils.data.DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch: 10/10 ... Train Loss: 0.0151  ... Validation Loss: 0.2852\n"
     ]
    }
   ],
   "source": [
    "num_cat = 2    \n",
    "exclude_processing = np.zeros(len(neural_df['unit'].unique()))\n",
    "exclude_processing[-num_cat:] = np.ones(num_cat)\n",
    "exclude_processing = exclude_processing.astype(bool)\n",
    "\n",
    "\n",
    "#Define hyperparameters\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-2\n",
    "hidden_dim = 100\n",
    "dropout = 0.5\n",
    "n_layers = 2\n",
    "max_epochs = 10\n",
    "input_size = training_set.X_tensor[0].shape[1] \n",
    "output_size = training_set.y_tensor[0].shape[1] \n",
    "\n",
    "\n",
    "# model_rnn = mocap_functions.model_gru(input_size, output_size, hidden_dim, n_layers, dropout, device).to(device)\n",
    "model_rnn = model_gru(input_size, output_size, hidden_dim, n_layers, dropout, device, cat_features=exclude_processing).to(device)\n",
    "\n",
    "# Define Loss, Optimizerints h\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model_rnn.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "#Train model\n",
    "loss_dict = mocap_functions.train_validate_model(model_rnn, optimizer, criterion, max_epochs, training_generator, validation_generator, device, 10, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate trained model\n",
    "rnn_train_pred = mocap_functions.evaluate_model(model_rnn, training_eval_generator, device)\n",
    "rnn_test_pred = mocap_functions.evaluate_model(model_rnn, testing_generator, device)\n",
    "\n",
    "#Evaluate trained model\n",
    "rnn_train_pred = mocap_functions.evaluate_model(model_rnn, training_eval_generator, device)\n",
    "rnn_test_pred = mocap_functions.evaluate_model(model_rnn, testing_generator, device)\n",
    "\n",
    "# rnn_train_corr = mocap_functions.matrix_corr(rnn_train_pred, y_train_data)\n",
    "# rnn_test_corr = mocap_functions.matrix_corr(rnn_test_pred, y_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SEE_Dataset_packed at 0x7f1ac3373400>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_generator.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ntolley/Donoghue_Lab/SEE_decoding/notebooks/pack_sequences.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ntolley/Donoghue_Lab/SEE_decoding/notebooks/pack_sequences.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model_rnn(pad_collate(testing_set[:])[\u001b[39m0\u001b[39m])\n",
      "\u001b[1;32m/home/ntolley/Donoghue_Lab/SEE_decoding/notebooks/pack_sequences.ipynb Cell 24\u001b[0m in \u001b[0;36mpad_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ntolley/Donoghue_Lab/SEE_decoding/notebooks/pack_sequences.ipynb#X46sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpad_collate\u001b[39m(batch):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ntolley/Donoghue_Lab/SEE_decoding/notebooks/pack_sequences.ipynb#X46sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     (xx, yy) \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ntolley/Donoghue_Lab/SEE_decoding/notebooks/pack_sequences.ipynb#X46sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     x_lens \u001b[39m=\u001b[39m [\u001b[39mlen\u001b[39m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m xx]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ntolley/Donoghue_Lab/SEE_decoding/notebooks/pack_sequences.ipynb#X46sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     y_lens \u001b[39m=\u001b[39m [\u001b[39mlen\u001b[39m(y) \u001b[39mfor\u001b[39;00m y \u001b[39min\u001b[39;00m yy]\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "model_rnn(pad_collate(testing_set[:])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rnn(pred_df, neural_df, neural_offset, task_info=True, window_size=10, num_cat=0):\n",
    "  if task_info:\n",
    "    exclude_processing = np.zeros(len(neural_df['unit'].unique()))\n",
    "    exclude_processing[-num_cat:] = np.ones(num_cat)\n",
    "    exclude_processing = exclude_processing.astype(bool)\n",
    "  else:\n",
    "    exclude_processing = None\n",
    "\n",
    "  data_arrays, generators = mocap_functions.make_generators(\n",
    "    pred_df, neural_df, neural_offset, cv_dict, metadata, exclude_neural=exclude_processing,\n",
    "    window_size=window_size, flip_outputs=True)\n",
    "  \n",
    "  # Unpack tuple into variables\n",
    "  training_set, validation_set, testing_set = data_arrays\n",
    "  training_generator, training_eval_generator, validation_generator, testing_generator = generators\n",
    "\n",
    "  X_train_data = training_set[:][0][:,-1,:].detach().cpu().numpy()\n",
    "  y_train_data = training_set[:][1][:,-1,:].detach().cpu().numpy()\n",
    "\n",
    "  X_test_data = testing_set[:][0][:,-1,:].detach().cpu().numpy()\n",
    "  y_test_data = testing_set[:][1][:,-1,:].detach().cpu().numpy()\n",
    "\n",
    "  #Define hyperparameters\n",
    "  lr = 1e-4\n",
    "  weight_decay = 1e-2\n",
    "  hidden_dim = 100\n",
    "  dropout = 0.5\n",
    "  n_layers = 2\n",
    "  max_epochs = 1000\n",
    "  input_size = training_set[0][0].shape[1] \n",
    "  output_size = training_set[0][1].shape[1] \n",
    "\n",
    "\n",
    "  # model_rnn = mocap_functions.model_gru(input_size, output_size, hidden_dim, n_layers, dropout, device).to(device)\n",
    "  model_rnn = model_gru(input_size, output_size, hidden_dim, n_layers, dropout, device, cat_features=exclude_processing).to(device)\n",
    "\n",
    "  # Define Loss, Optimizerints h\n",
    "  criterion = nn.MSELoss()\n",
    "  optimizer = torch.optim.Adam(model_rnn.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "  #Train model\n",
    "  loss_dict = mocap_functions.train_validate_model(model_rnn, optimizer, criterion, max_epochs, training_generator, validation_generator, device, 10, 5)\n",
    "\n",
    "  #Evaluate trained model\n",
    "  rnn_train_pred = mocap_functions.evaluate_model(model_rnn, training_eval_generator, device)\n",
    "  rnn_test_pred = mocap_functions.evaluate_model(model_rnn, testing_generator, device)\n",
    "\n",
    "  #Evaluate trained model\n",
    "  rnn_train_pred = mocap_functions.evaluate_model(model_rnn, training_eval_generator, device)\n",
    "  rnn_test_pred = mocap_functions.evaluate_model(model_rnn, testing_generator, device)\n",
    "\n",
    "  rnn_train_corr = mocap_functions.matrix_corr(rnn_train_pred, y_train_data)\n",
    "  rnn_test_corr = mocap_functions.matrix_corr(rnn_test_pred, y_test_data)\n",
    "\n",
    "  return rnn_train_pred, rnn_test_pred, rnn_train_corr, rnn_test_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0efaeb1f57b331682f483338b2ece2b224a46046d22ccab062f7cb485953279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
