{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntolley/anaconda3/envs/SEE_vision/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../code') \n",
    "import mocap_functions\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import scipy.io as sio\n",
    "import multiprocessing\n",
    "import Neural_Decoding\n",
    "from torch import nn\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/SPK20220308/task_data/'\n",
    "kinematic_df, neural_df, metadata = mocap_functions.load_mocap_df(data_path)\n",
    "video_df = pd.read_pickle(data_path + 'video_df.pkl')\n",
    "\n",
    "\n",
    "num_trials = metadata['num_trials']\n",
    "\n",
    "#Generate cv_dict for regular train/test/validate split (no rolling window)\n",
    "cv_split = ShuffleSplit(n_splits=5, test_size=.25, random_state=0)\n",
    "val_split = ShuffleSplit(n_splits=1, test_size=.25, random_state=0)\n",
    "cv_dict = {}\n",
    "for fold, (train_val_idx, test_idx) in enumerate(cv_split.split(np.arange(num_trials))):\n",
    "    for t_idx, v_idx in val_split.split(train_val_idx): #No looping, just used to split train/validation sets\n",
    "        cv_dict[fold] = {'train_idx':train_val_idx[t_idx], \n",
    "                         'test_idx':test_idx, \n",
    "                         'validation_idx':train_val_idx[v_idx]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dataframes add or remove layout info\n",
    "nolayout_neural_mask = ~(neural_df['unit'].str.contains(pat='layout'))\n",
    "noposition_neural_mask = ~(neural_df['unit'].str.contains(pat='position'))\n",
    "neural_df = neural_df[np.logical_and(nolayout_neural_mask, noposition_neural_mask)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = torchvision.io.VideoReader('../data/SPK20220308/videos/SpikeCam1_03-08-1557.mp4', 'video')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "analog_sampling_rate = 1000\n",
    "cam_frames = np.load('cam_frames.npy')\n",
    "\n",
    "fpath = '../data/SPK20220308/task_data/'\n",
    "experiment_dict = sio.loadmat(f'{fpath}eventsCB1_corrected.mat')\n",
    "ev_ex = experiment_dict['eventsCB1']\n",
    "\n",
    "cam_start = cam_frames[0] / analog_sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 "
     ]
    }
   ],
   "source": [
    "for idx in range(60):\n",
    "    print(idx, end=' ')\n",
    "    reader.seek(video_df['frames'][0][idx] - cam_start)\n",
    "    frame = next(reader)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(frame['data'].numpy()[0,:,:])\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    pooler = torch.nn.AvgPool2d(kernel_size=100, stride=50)\n",
    "    frame_pooled = pooler(frame['data'].float())\n",
    "    plt.imshow(frame_pooled[0,:,:])\n",
    "\n",
    "    plt.savefig(f'../data/SPK20220308/frames/frame{idx}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1000, 500) to (1008, 512) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    }
   ],
   "source": [
    "images = [f'frame{idx}.png' for idx in range(60)]\n",
    "mocap_functions.make_movie('../data/SPK20220308/frames/', 'pooling_layer.mp4', images, fps=30, quality=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class to handle mocap dataframes from SEE project\n",
    "class SEE_Video_Dataset(torch.utils.data.Dataset):\n",
    "    #'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, cv_dict, fold, partition, video_df, neural_df, reader,\n",
    "                 offset, window_size, data_step_size, device, cam_start,\n",
    "                 scale_neural=True, flip_outputs=False, scaler=None):\n",
    "        #'Initialization'\n",
    "        self.cv_dict = cv_dict\n",
    "        self.fold = fold\n",
    "        self.flip_outputs = flip_outputs\n",
    "        self.partition = partition\n",
    "        self.trial_idx = cv_dict[fold][partition]\n",
    "        self.num_trials = len(self.trial_idx) \n",
    "        self.offset = offset\n",
    "        self.window_size = window_size\n",
    "        self.data_step_size = data_step_size\n",
    "        self.device = device\n",
    "        self.videoData_list, self.neuralData_list = self.process_dfs(video_df, neural_df)\n",
    "        self.pooler = torch.nn.AvgPool2d(kernel_size=100, stride=50)\n",
    "        #self.pooler = torch.nn.AvgPool2d(kernel_size=10, stride=5)\n",
    "        self.reader = reader\n",
    "        self.cam_start = cam_start\n",
    "\n",
    "        if scaler is None:\n",
    "            self.scaler = StandardScaler()\n",
    "        else:\n",
    "            self.scaler = scaler\n",
    "        \n",
    "        if scale_neural:\n",
    "            self.neuralData_list = self.transform_data(self.neuralData_list)\n",
    "\n",
    "        self.split_offset = np.round((self.offset/self.data_step_size) / 2).astype(int)\n",
    "\n",
    "        self.X_tensor, self.y_tensor = self.load_splits()\n",
    "        self.num_samples = np.sum(self.X_tensor.size(0))\n",
    "\n",
    "    def __len__(self):\n",
    "        #'Denotes the total number of samples'\n",
    "        return self.num_samples\n",
    "\n",
    "    # Need to modify if flipouts is ever enabled\n",
    "    def __getitem__(self, slice_index):\n",
    "        frame_data = list()\n",
    "        frame_times = self.X_tensor[slice_index, :, :]\n",
    "        for ftime in frame_times:\n",
    "            self.reader.seek(ftime - self.cam_start)\n",
    "            frame = next(self.reader)\n",
    "            frame_pooled = self.pooler(frame['data'].float())\n",
    "            frame_data.append(frame_pooled[0, :, :].reshape(-1))\n",
    "\n",
    "        frame_data = torch.vstack(frame_data)\n",
    "        return frame_data, self.y_tensor[slice_index,:,:]\n",
    "\n",
    "    def process_dfs(self, video_df, neural_df):\n",
    "        frame_list, neuralData_list = list(), list()\n",
    "        for trial in self.trial_idx:\n",
    "            frame_array = np.stack(video_df['frames'][video_df['trial'] == trial].values).transpose() \n",
    "            neuralData_array = np.stack(neural_df['rates_video'][neural_df['trial'] == trial].values).squeeze().transpose() \n",
    "\n",
    "            frame_list.append(frame_array)\n",
    "            neuralData_list.append(neuralData_array)\n",
    "\n",
    "        return frame_list, neuralData_list\n",
    "\n",
    "    # **START HERE**, need to check if frame list is the right shape for unfold\n",
    "    # Hold of torchvision reader seek until end, but should still be able to unfold frame times\n",
    "    def format_splits(self, data_list):\n",
    "        unfolded_data_list = list()\n",
    "        for trial_idx in range(self.num_trials):\n",
    "            unfolded_trial = torch.from_numpy(data_list[trial_idx]).unfold(0, self.window_size, self.data_step_size).transpose(1, 2)\n",
    "            unfolded_data_list.append(unfolded_trial)\n",
    "        \n",
    "        data_tensor = torch.concat(unfolded_data_list, axis=0)\n",
    "\n",
    "        return data_tensor\n",
    "    \n",
    "    def load_splits(self):\n",
    "        if not self.flip_outputs:\n",
    "            X_tensor = self.format_splits(self.videoData_list)\n",
    "            y_tensor = self.format_splits(self.neuralData_list)\n",
    "        else:\n",
    "            y_tensor = self.format_splits(self.videoData_list)\n",
    "            X_tensor = self.format_splits(self.neuralData_list)\n",
    "\n",
    "        X_tensor, y_tensor = X_tensor[:-self.split_offset,::self.data_step_size,:], y_tensor[self.split_offset:,::self.data_step_size,:]\n",
    "\n",
    "        assert X_tensor.shape[0] == y_tensor.shape[0]\n",
    "        return X_tensor, y_tensor\n",
    "\n",
    "    #Zero mean and unit std\n",
    "    def transform_data(self, data_list):\n",
    "        #Iterate over trials and apply normalization\n",
    "     \n",
    "        scaled_data_list = []\n",
    "        for data_trial in data_list:\n",
    "            scaled_data_trial = self.scaler.fit_transform(data_trial)\n",
    "            scaled_data_list.append(scaled_data_trial)\n",
    "\n",
    "        return scaled_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up PyTorch Dataloaders\n",
    "fold=0\n",
    "scale_neural = True\n",
    "flip_outputs = False\n",
    "device = 'cpu'\n",
    "\n",
    "data_step_size = 1\n",
    "# Parameters\n",
    "batch_size = 10000\n",
    "train_params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': num_cores, 'pin_memory':False}\n",
    "train_eval_params = {'batch_size': batch_size, 'shuffle': False, 'num_workers': num_cores, 'pin_memory':False}\n",
    "validation_params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': num_cores, 'pin_memory':False}\n",
    "test_params = {'batch_size': batch_size, 'shuffle': False, 'num_workers': num_cores, 'pin_memory':False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = SEE_Video_Dataset(cv_dict=cv_dict, fold=fold, partition='train_idx', video_df=video_df,\n",
    "                                 neural_df=neural_df, reader=reader, offset=offset, window_size=window_size,\n",
    "                                 data_step_size=data_step_size, device=device, cam_start=cam_start,\n",
    "                                 scale_neural=scale_neural, flip_outputs=flip_outputs, scaler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generators_video(video_df, neural_df, offset, cv_dict, window_size=1, device='cpu'):\n",
    "    # Set up PyTorch Dataloaders\n",
    "    fold=0\n",
    "    scale_neural = True\n",
    "    flip_outputs = False\n",
    "    device = 'cpu'\n",
    "\n",
    "    window_size = 2\n",
    "    data_step_size = 1\n",
    "    \n",
    "    # Parameters\n",
    "    batch_size = 10000\n",
    "    train_params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': num_cores, 'pin_memory':False}\n",
    "    train_eval_params = {'batch_size': batch_size, 'shuffle': False, 'num_workers': num_cores, 'pin_memory':False}\n",
    "    validation_params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': num_cores, 'pin_memory':False}\n",
    "    test_params = {'batch_size': batch_size, 'shuffle': False, 'num_workers': num_cores, 'pin_memory':False}\n",
    "\n",
    "\n",
    "    # Generators\n",
    "    training_set = SEE_Video_Dataset(cv_dict=cv_dict, fold=fold, partition='train_idx', video_df=video_df,\n",
    "                                 neural_df=neural_df, reader=reader, offset=offset, window_size=window_size,\n",
    "                                 data_step_size=data_step_size, device=device, cam_start=cam_start,\n",
    "                                 scale_neural=scale_neural, flip_outputs=flip_outputs, scaler=None)\n",
    "    training_generator = torch.utils.data.DataLoader(training_set, **train_params)\n",
    "    training_eval_generator = torch.utils.data.DataLoader(training_set, **train_eval_params)\n",
    "\n",
    "    validation_set = SEE_Video_Dataset(cv_dict=cv_dict, fold=fold, partition='validation_idx', video_df=video_df,\n",
    "                                 neural_df=neural_df, reader=reader, offset=offset, window_size=window_size,\n",
    "                                 data_step_size=data_step_size, device=device, cam_start=cam_start,\n",
    "                                 scale_neural=scale_neural, flip_outputs=flip_outputs, scaler=None)\n",
    "\n",
    "    validation_generator = torch.utils.data.DataLoader(validation_set, **validation_params)\n",
    "\n",
    "    testing_set = SEE_Video_Dataset(cv_dict=cv_dict, fold=fold, partition='test_idx', video_df=video_df,\n",
    "                                 neural_df=neural_df, reader=reader, offset=offset, window_size=window_size,\n",
    "                                 data_step_size=data_step_size, device=device, cam_start=cam_start,\n",
    "                                 scale_neural=scale_neural, flip_outputs=flip_outputs, scaler=None)\n",
    "\n",
    "    testing_generator = torch.utils.data.DataLoader(testing_set, **test_params)\n",
    "\n",
    "    data_arrays = (training_set, validation_set, testing_set)\n",
    "    generators = (training_generator, training_eval_generator, validation_generator, testing_generator)\n",
    "\n",
    "    return data_arrays, generators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_wiener_video(video_df, neural_df, offset, cv_dict, window_size):\n",
    "    data_arrays, generators = make_generators_video(video_df, neural_df, offset, cv_dict, window_size=30)\n",
    "\n",
    "    # Unpack tuple into variables\n",
    "    training_set, validation_set, testing_set = data_arrays\n",
    "    training_generator, training_eval_generator, validation_generator, testing_generator = generators\n",
    "\n",
    "    X_train_data = training_set[:][0][:,-1,:].detach().cpu().numpy()\n",
    "    y_train_data = training_set[:][1][:,-1,:].detach().cpu().numpy()\n",
    "\n",
    "    X_test_data = testing_set[:][0][:,-1,:].detach().cpu().numpy()\n",
    "    y_test_data = testing_set[:][1][:,-1,:].detach().cpu().numpy()\n",
    "\n",
    "    #Fit and run wiener filter\n",
    "    model_wr = Neural_Decoding.decoders.WienerFilterDecoder() \n",
    "    model_wr.fit(X_train_data,y_train_data)\n",
    "\n",
    "    wr_train_pred = model_wr.predict(X_train_data)\n",
    "    wr_test_pred = model_wr.predict(X_test_data)\n",
    "\n",
    "    #Compute decoding\n",
    "    wr_train_corr = mocap_functions.matrix_corr(wr_train_pred,y_train_data)\n",
    "    wr_test_corr = mocap_functions.matrix_corr(wr_test_pred,y_test_data)\n",
    "\n",
    "    return wr_train_pred, wr_test_pred, wr_train_corr, wr_test_corr\n",
    "\n",
    "def run_ann_video(video_df, neural_df, offset, cv_dict, window_size):\n",
    "    data_arrays, generators = make_generators_video(video_df, neural_df, offset, cv_dict, window_size=30)\n",
    "\n",
    "    # Unpack tuple into variables\n",
    "    X_train_data, y_train_data, X_test_data, y_test_data = data_arrays\n",
    "    training_generator, training_eval_generator, validation_generator = generators\n",
    "\n",
    "    #Define hyperparameters\n",
    "    lr = 1e-3\n",
    "    weight_decay = 1e-5\n",
    "    layer_size=[100]\n",
    "    max_epochs=1000\n",
    "    input_size = X_train_data.shape[1] \n",
    "    output_size = y_train_data.shape[1] \n",
    "\n",
    "    model_ann = mocap_functions.model_ann(input_size,output_size,layer_size).to(device)\n",
    "    # Define Loss, Optimizerints h\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model_ann.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    #Train model\n",
    "    loss_dict = mocap_functions.train_validate_model(model_ann, optimizer, criterion, max_epochs, training_generator, validation_generator, device, 10, 5)\n",
    "\n",
    "    #Evaluate trained model\n",
    "    ann_train_pred = mocap_functions.evaluate_model(model_ann, training_eval_generator, device)\n",
    "    ann_test_pred = mocap_functions.evaluate_model(model_ann, validation_generator, device)\n",
    "    \n",
    "    #Compute decoding\n",
    "    ann_train_corr = mocap_functions.matrix_corr(ann_train_pred,y_train_data)\n",
    "    ann_test_corr = mocap_functions.matrix_corr(ann_test_pred,y_test_data)\n",
    "\n",
    "    return ann_train_pred, ann_test_pred, ann_train_corr, ann_test_corr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "seek() Expected a value of type 'float' for argument '_1' but instead found type 'Tensor'.\nPosition: 1\nValue: tensor([[3694.5890],\n        [3694.6120]], dtype=torch.float64)\nDeclaration: seek(__torch__.torch.classes.torchvision.Video _0, float _1, bool _2) -> NoneType _0\nCast error details: Unable to cast Python instance to C++ type (#define PYBIND11_DETAILED_ERROR_MESSAGES or compile in debug mode for details)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8402/3589707590.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdecode_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfunc_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdf_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_df\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtrain_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_corr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_corr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneural_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         decode_results[func_name][df_type] = {\n",
      "\u001b[0;32m/tmp/ipykernel_8402/1767149613.py\u001b[0m in \u001b[0;36mrun_wiener_video\u001b[0;34m(video_df, neural_df, offset, cv_dict, window_size)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtraining_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_eval_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mX_train_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0my_train_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8402/2395801265.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, slice_index)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mframe_times\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslice_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mftime\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mframe_times\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mftime\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcam_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mframe_pooled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SEE_vision/lib/python3.7/site-packages/torchvision/io/video_reader.py\u001b[0m in \u001b[0;36mseek\u001b[0;34m(self, time_s, keyframes_only)\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mthe\u001b[0m \u001b[0mfirst\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtimestamp\u001b[0m \u001b[0mlarger\u001b[0m \u001b[0mthan\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtime_s\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \"\"\"\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyframes_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: seek() Expected a value of type 'float' for argument '_1' but instead found type 'Tensor'.\nPosition: 1\nValue: tensor([[3694.5890],\n        [3694.6120]], dtype=torch.float64)\nDeclaration: seek(__torch__.torch.classes.torchvision.Video _0, float _1, bool _2) -> NoneType _0\nCast error details: Unable to cast Python instance to C++ type (#define PYBIND11_DETAILED_ERROR_MESSAGES or compile in debug mode for details)"
     ]
    }
   ],
   "source": [
    "window_size = 2\n",
    "offset = 2\n",
    "\n",
    "#func_dict = {'wiener': run_wiener_video, 'rnn': run_ann_video}\n",
    "func_dict = {'wiener': run_wiener_video}\n",
    "\n",
    "df_dict = {'video': {'df': video_df}}\n",
    "\n",
    "decode_results = dict()\n",
    "for func_name, func in func_dict.items():\n",
    "    decode_results[func_name] = dict()\n",
    "    for df_type, pred_df in df_dict.items():\n",
    "        train_pred, test_pred, train_corr, test_corr = func(video_df, neural_df, offset, cv_dict, window_size)\n",
    "\n",
    "        decode_results[func_name][df_type] = {\n",
    "            'train_pred': train_pred,\n",
    "            'test_pred': test_pred,\n",
    "            'train_corr': train_corr,\n",
    "            'test_corr': test_corr\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SEE_vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a246e07bf8b66b5c1880356d8a3e70a4d9601ee6ceb6631567362f046b2dec5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
